# BCI-Resources

All resources related to BCI

# EEG (Electroencephalogram)

## Datasets

| Study (Year)                            | Dataset (Images × Classes)                                                                                                                                                            | EEG System (Channels, Rate)                                                                                      | Model & Methodology                                                                                                                                                                                                                                | Performance (Accuracy)                                                                                                                                                                                                                       | Code/Public Data                                                             |
| --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| **Spampinato et al., CVPR 2017**        | 2,000 images (40 object classes from ImageNet; 6 subjects) – *“EEG CVPR40”*.                                                                                                          | BrainProducts actiCAP, 128 active channels @1 kHz (wet electrodes, gel).                                         | CNN + LSTM “Deep Learning Human Mind” network (RNN-based EEG feature extractor feeding an image classifier).                                                                                                                                       | ~40% mean accuracy in 40-way classification (subject-dependent). When combining EEG with the image CNN, reported up to ~83% (due to image features aiding classification).                                                                   | *Dataset & code:* Available via perceivelab (Univ. of Catania).              |
| **Palazzo et al., TPAMI 2020**          | Same 40-class EEG dataset as above (extension of CVPR’17).                                                                                                                            | 128-ch BrainProducts system (as above).                                                                          | Triplet-loss metric learning to embed EEG in image feature space; CNN-LSTM hybrid encoders. Improvements on Spampinato 2017.                                                                                                                       | Reported 82.9% accuracy by combining EEG with image features (pure EEG ~**20–40%** range). Improved feature generalization to unseen classes.                                                                                                | – (Uses same data as 2017; code integrated in above)                         |
| **Gifford et al., NeuroImage 2022**     | **THINGS-EEG**: 82,160 trials from 16,740 images (1,854 object concepts) across 10 subjects. Rapid 5 Hz serial visual presentation (RSVP) paradigm.                                   | BrainVision actiCHamp amp + 64-ch EasyCap @1 kHz (10–10 layout, wet).                                            | Linear **encoding models**: learned to predict EEG responses from image features; also zero-shot object *identification* by matching EEG to image embeddings.                                                                                      | ~**81.35%** accuracy identifying the correct image among 200 candidates; ~21% accuracy among 150k candidates (chance 0.0007). (Extrapolated ~10% for 4.5M images) – far above chance.                                                        | **OpenNeuro** data release (BIDS format); code on OSF.                       |
| **Song et al., ICLR 2024** (“NICE-EEG”) | Same THINGS-EEG (10 subj subset) – used 16,540 images for training, 200 images for zero-shot test.                                                                                    | 64-ch BrainProducts @1 kHz (full 64 used; downsampled to 250 Hz for analysis).                                   | **Contrastive self-supervised** framework: ResNet/ViT image encoder + EEG Transformer encoder; align EEG and image embeddings in a shared space (inspired by CLIP). Attention modules to capture spatial EEG correlations.                         | **Top-1 15.6%**, top-5 42.8% accuracy in 200-way zero-shot image identification (chance 0.5%). SOTA on this dataset, outperforming prior BraVL by ~8%. Also robust across time, frequency and subjects.                                      | **Code:** “NICE-EEG” on GitHub.                                              |
| **Du et al., TPAMI 2023** (“BraVL”)     | EEG recordings for 40 ImageNet classes (extended from Spampinato dataset) + text labels (for multimodal learning). Also tested zero-shot on *novel* classes.                          | 128-ch EEG (from Spampinato 2017 dataset).                                                                       | **Brain-Visual-Linguistic** multimodal AE: learns a joint latent for EEG, image, and text. Uses *mixture-of-experts* multimodal VAE with mutual information regularization. Leverages word embeddings (text) to help generalize to unseen classes. | **Zero-shot object classification** on novel classes achieved significantly above chance. (BraVL’s 200-way accuracy ≈5–7% top-1 in Song et al.’s evaluation, vs 13.8% by NICE-EEG.) Multimodal fusion (visual+linguistic) gave best results. | **Code:** GitHub “ChangdeDu/BraVL” (open-source).                            |
| **MindBigData (Vivancos, 2019–2023)**   | **MindBigData** “MNIST-EEG” (open dataset): >1.2 M EEG trials of 2 s each while subjects viewed digits 0–9. Collected with low-cost headsets (NeuroSky, Emotiv, Muse: 1–14 channels). | Emotiv Insight & EPOC (5-ch, 14-ch), Muse (4-ch), NeuroSky (1-ch). Mixed sampling (~128–256 Hz). Dry electrodes. | Traditional ML (XGBoost, SVM) and simple CNNs used as benchmarks for digit classification. Emphasis on large-scale data for deep learning.                                                                                                         | Low but above-chance digit classification accuracy given consumer devices (e.g. ~20–30% for 10-way digits vs 10% chance, depending on model) – a baseline for “EEG MNIST”.                                                                   | **Data:** Publicly available (Kaggle, HuggingFace). Code examples on GitHub. |


## Models

| Study (Year)                                       | Approach                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | EEG/Data Setup                                                                                                                                                               | Key Results (Reconstructions)                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Code / Repo                                                            |
| -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| **Tirupattur et al., ACM MM 2018** (“ThoughtViz”)  | **GAN-based**: Trained a conditional GAN to *“visualize human thoughts”* from EEG features. Two-stage pipeline: (1) train an EEG classifier to predict class labels; (2) use EEG-derived features as input to a GAN to generate an image for that class.                                                                                                                                                                                                                                                      | Used a public EEG dataset of 10 object classes (BCI competition IV-2a imagery dataset) – 14-channel EEG, 23 subjects. Emotiv EPOC headset for EEG signals (sampling 128 Hz). | Generated rough image outputs for each category (e.g. outlines of a face vs a house). While low-fidelity, the GAN outputs were semantically recognizable as the target class in many cases. Classification step reached ~**70–75%** on 2-class EEG classification and provided embeddings for generation.                                                                                                                                                                             | **Code:** Available on GitHub (“ptirupat/ThoughtViz”).                 |
| **Li et al., NeurIPS 2024** (“ATM” by Liu’s group) | **Diffusion-based** zero-shot framework: An **Adaptive Thinking Mapper (ATM)** network first maps EEG into a CLIP image embedding space. Then a two-stage *guided diffusion* model generates images: Stage 1 produces a coarse “blurry” image and refined image priors from the EEG embedding; Stage 2 uses a pre-trained diffusion model (Stable Diffusion) conditioned on the EEG embedding *and an auto-generated text caption* to synthesize a final image. EEG features also guide low-level attributes. | THINGS-EEG dataset (10 subjects, 64-ch, 1000 Hz) – same as above. Focused on 200 held-out images for testing (zero-shot). Also evaluated on THINGS-MEG for 4 subjects.       | **Classification**: ATM achieved **Top-1 ~15%** on 200-way EEG-image retrieval (vs ~14% by prior BraVL). **Image Generation**: Produced recognizable images aligning with true stimuli (e.g. EEG of a giraffe yields an animal-like shape with a long neck). Quantitatively, ATM’s generated images had higher semantic similarity to targets than GAN baselines (e.g. +12.7% higher retrieval score over BraVL). Human evaluators could often guess the category from ATM’s outputs. | **Code:** Open-sourced by SUSTech-NCC Lab (GitHub “EEG_Image_decode”). |
| **Cao et al., Neural Netw 2025** (“EEG-CLIP”)      | **Transformer & Diffusion Prior**: Introduced an **EEG-ViT encoder** to capture spatio-temporal EEG features and a **Diffusion Prior** network to map EEG features to image latent space. Uses a dual-stage pipeline similar to above: contrastive learning to project EEG into CLIP image feature space, then a pre-trained conditional diffusion model (Stable Diffusion) generates the image. Extensive evaluation protocols (temporal sensitivity, regional activation analysis) were proposed.           | Trained on **Things-EEG** and a smaller **Brain2Image** dataset (40 classes, 128-ch EEG). EEG-CLIP used high-density 128-channel caps (BrainVision or EGI) at 500–1000 Hz.   | Achieved **state-of-the-art** image reconstructions and classification: e.g. **64%** top-1 EEG object classification on EEG-ImageNet 40-class (vs ~40% in prior work) after fine-tuning on that dataset. Reconstruction fidelity improved substantially (Inception Score +62.9% on EEGCVPR40 vs prior StyleGAN method). Generated images had more detail and fewer artifacts, confirmed by better FID/KID scores.                                                                     | **Code:** “EEG-CLIP” code to be released (per authors).                |


## Devices

# fMRI (functional Magnetic Resonance Imaging)

## Datasets

| **Study / Dataset (Year)**                                                                                                                                                                                                  | **MRI Setup** (Field, TR, Voxels, Coil)                  | **Visual ROI**                     | **Decoding Method**                                                                                                                  | **Performance (Metrics)**                                                                                                                                                                   | **Code / Data**                                   |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------- | ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |
| **Natural Scenes Dataset (NSD)** – Allen et al. (2022)                                                                                                                                                                      | 7 T Siemens, TR 1.6 s, 1.8 mm iso; 32ch head coil (CMRR) | Whole brain (focus on V1–IT)       | fMRI encoding models for DNN features; benchmark dataset                                                                             | 73,000 trials, rich scene stimuli; enabled new SOTA models                                                                                                                                  | [NSD Public Dataset]                              |
| **BOLD5000** – Chang et al. (2019)                                                                                                                                                                                          | 3 T Siemens Verio, TR 2 s, 2×2×2 mm; 32ch coil           | Ventral stream (V1–hV4, IT)        | Slow event design (5k natural images); used for CNN alignment and decoding benchmarks                                                | Image category decoding above chance; large-scale 5k images                                                                                                                                 | [BOLD5000 Data/Code]                              |
| **Generic Object Decoding (GOD)** – Horikawa & Kamitani (2017)                                                                                                                                                              | 3 T Siemens Verio, TR 2 s, 2×2×2 mm; custom 32ch coil    | V1–V4, LOC/IT (FFA, PPA)           | Linear **DNN feature decoders** (VGG features) from fMRI; identify seen & *imagined* object categories                               | ~95% top-2 identification (seen vs chance 50%); Decoded unseen categories beyond training                                                                                                   | Code + OpenNeuro data                             |
| **Deep Image Reconstruction** – Shen et al. (2019)                                                                                                                                                                          | 3 T Siemens Verio (same data as above)                   | V1–V4, HVC (higher visual)         | **Multi-layer DNN feature decoding** → iterative image optimization (with optional GAN prior)                                        | Reconstructed natural images resembling originals; human judges ~96–97% accuracy in pairwise image match. Imagery reconstructions rudimentary                                               | [GitHub: KamitaniLab/DeepImageReconstruction]     |
| **Self-supervised GAN Decoders** – Beliy et al. (2019); Gaziv et al. (2022)                                                                                                                                                 | 3 T (Kamitani data & others)                             | Visual cortex (V1–IT)              | **GAN-based** image generators trained with fMRI supervision + consistency losses                                                    | Sharper reconstructions than Shen’19; improved semantic accuracy (e.g. ≳ 15% top-100 accuracy)                                                                                              | (Beliy’19 & Gaziv’22 code via authors)            |
| **BigBiGAN & VAE/GAN hybrids** – Mozafari et al. (2020); Ren et al. (2021)                                                                                                                                                  | 3 T (Kamitani or similar datasets)                       | Visual cortex (V1–IT)              | **Bidirectional GAN** (BigBiGAN) focusing on semantic features; **Dual-VAE-GAN** with 3-stage training                               | Improved semantic fidelity (e.g. clearer object class); Ren’21 boosted realism via adversarial training                                                                                     | (Author code on request)                          |
| **Stable Diffusion Recon** – Takagi & Nishimoto (2023)                                                                                                                                                                      | 3 T (Osaka University; NSD subset)                       | Early & high-level visual areas    | **Latent Diffusion (LDM)** with **no retraining** – map fMRI to Stable Diffusion’s latent and CLIP text embedding                    | High-res reconstructions with accurate semantics; ~74.3% two-alternative identification accuracy on NSD (chance 50%)                                                                        | [GitHub: yu-takagi/StableDiffusionReconstruction] |
| **Brain-Diffuser** – Ozcelik & VanRullen (2023)                                                                                                                                                                             | 7 T (NSD benchmark data)                                 | Whole brain (NSD; ROI analyses)    | **Two-stage**: fMRI → **VDVAE** coarse image; then Versatile Diffusion (image+text) refinement                                       | Outperforms prior models on NSD scenes qualitatively & quantitatively (higher SSIM and recognition scores); ROI-wise “optimal” images reflect expected category bias (e.g. PPA → buildings) | [Scientific Reports 2023 open access]             |
| **MinD-Vis** – Chen et al. (CVPR 2023)                                                                                                                                                                                      | 3 T (Generic Object Decoding + BOLD5000)                 | Visual cortex (occipital–temporal) | **Masked brain modeling + double-conditioned diffusion** (fMRI → LDM with twin conditioning); pre-trained on 16k resting-state scans | **State-of-the-art** on GOD dataset: 23.9% Top-1 accuracy in 100-way image classification (⇧66% vs prior); FID 1.67 (⇧41% better). Good cross-dataset generalization (tested on BOLD5000)   | [GitHub: mind-vis]                                |
| *Other notable:* **DreamDiffusion** (Yang et al. 2024) – EEG-to-image diffusion (built on fMRI decoding advances); **Algonauts 2019–23** – community fMRI challenges (EVC/IT image set; NSD videos) for model benchmarking. |      
|                                    |                               
## Models

## Devices

# fNIRS (functional Near-Infrared Spectroscopy)

## Datasets

| **Dataset (Year)**                          | **Paradigm / Tasks**                                                                       | **Subjects** | **Notes**                                                                                                                                                                                                                    |
| ------------------------------------------- | ------------------------------------------------------------------------------------------ | ------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Tufts fNIRS2MW** (Wang et al. 2021)       | *N-back* working-memory (0–3-back) for cognitive workload                                  | 68           | Forehead fNIRS (8 channels at 5.2 Hz) during controlled n-back tasks (open dataset, largest of its kind). Supports standardized train/test splits for workload classifiers.                                                  |
| **Sujin Bak et al. 2019**                   | *Motor*: Unilateral finger-tapping (left vs right) and foot-tapping (3-class overt motor)  | 30           | Open fNIRS dataset (figshare) for classification of hand/foot movements. Achieved ~70% 3-class accuracy with SVM; often used as a benchmark for BCI models.                                                                  |
| **Lower Limb MI 2025** (Nazeer et al. 2025) | *Motor imagery*: Ankle dorsiflexion/plantarflexion and knee flexion/extension (lower-limb) | 21           | Newly released fNIRS dataset for rehab BCIs. Recorded with NIRSport2 (8 sources × 8 detectors, 20 channels over motor cortex) at 10.2 Hz. Targets exoskeleton control research.                                              |
| **NASA Astronaut Cognitive** (2020)         | *Vigilance/Operational*: Astronaut simulator tasks, cognitive state monitoring             | N/A          | NASA released fNIRS data for real-time monitoring of cognitive state decline during space operations. Data available via NASA (application required). Illustrates PVT-like vigilance assessment in high-stakes environments. |
| **PhysioNet Artifact 2014**                 | *Artifact simulation*: Rest with motion artifacts (EEG+fNIRS)                              | 6 sessions   | Shared via PhysioNet: simultaneous fNIRS & EEG recordings contaminated with deliberate motion artifacts. Used to validate artifact removal methods.                                                                          |
| **Hybrid BCI (TU Berlin) 2017**             | *Hybrid mental tasks*: e.g. mental arithmetic, word generation (EEG + fNIRS)               | 16           | Open hybrid dataset (TU Berlin) with concurrent EEG-fNIRS during multiple cognitive tasks. Often used for multimodal fusion studies; fNIRS-only subset also extracted for analysis.                                          |


## Models

## Devices
| **Device**                  | **Type / Channels**                                           | **Key Features**                                                                                                             | **Notes**                                                                                                                                                                                |
| --------------------------- | ------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **NIRx NIRSport 2**         | Wearable multichannel; 8×8 or 16×16 optodes (64–128 channels) | Portable, battery-powered; uses LED sources; integrates with EEG caps. Sampling up to ~10 Hz.                                | Lightweight “portable lab” for mobile/BCI studies. Supports fast setup and even concurrent fMRI/TMS via fiber optics.                                                                    |
| **NIRx NIRScout**           | Lab-based modular; up to 64 sources × 128 detectors           | Highly configurable “gold-standard” fNIRS platform. Supports lasers or LEDs, short-separation channels, and 100 Hz sampling. | Enables high-density montages covering the whole cortex. Ideal for multi-modal integrations (EEG, eye-tracking, MRI) with trigger sync. Often used in cognitive neuroscience labs.       |
| **Artinis Brite** *(MKIII)* | Wearable wireless; up to 27 channels per unit (LED sources)   | Flexible optode configurations (e.g. 23 frontal, 27 visual, or dual 12-channel motor arrays). Max 100 Hz sampling.           | User-friendly plug-and-play device for field studies. Can combine two units (54 channels) for full-head coverage. Often used in neuroergonomics and VR attention studies.                |
| **Artinis OctaMon**         | Wearable pad; 8 channels (fixed)                              | Simple headband for prefrontal cortex (PFC) oxygenation. Uses 8 LEDs, 8 photodiodes; ~50 Hz.                                 | Optimized for frontal cognitive monitoring (e.g. workload, infant studies). Minimal setup, portable; also a muscle-oxygenation variant (OctaMon-M) exists.                               |
| **Hitachi ETG-4000**        | Bench-top system; 52 channels (18 lasers, 16 detectors)       | Continuous-wave system; two wavelengths (695/830 nm) at 10 Hz. Large console (~130 kg) for multi-channel recording.          | Widely used in early fNIRS studies of cognition. Cap layouts typically 3×11 optode grids per headset. Preconfigured probes for frontal, motor, etc. (Successor: ETG-7100 up to 120 ch).  |
| **OBELAB NIRSIT**           | High-density wearable; 204 channels (24 laser, 32 det.)       | Silicon pad covering forehead (prefrontal) with dense optode array. Sampling ~8 Hz. Portable at 450 g weight.                | Notable for its high density in a wearable form factor. Often used for cognitive load and psychiatric research in Korea. Newer **NIRSIT-ON** offers 8 channels at 32 Hz for simpler use. |



# MEG (Magnetoencephalography)

## Datasets

| **Dataset (Year)**        | **Subjects**  | **MEG System**          | **Visual Tasks**                                       | **Reference**           |
| ------------------------- | ------------- | ----------------------- | ------------------------------------------------------ | ----------------------- |
| **CamCAN (2017)**         | ~700 (18–87y) | Elekta VectorView 306ch | Sensorimotor (audio-visual stimuli), rest              | Taylor *et al.*, 2017   |
| **HCP-MEG (2018)**        | ~100 (young)  | 4D Magnes 248ch         | Working Memory (faces vs tools), Language, Motor, Rest | HCP MEG Manual          |
| **Wakeman-Henson (2015)** | 16            | Elekta Neuromag 306ch   | Passive viewing: Faces vs Scrambled faces (MEG+EEG)    | Wakeman & Henson 2015   |
| **DECAF (2015)**          | 30            | Elekta Triux 306ch      | Emotional faces (affective responses)                  | Abadi *et al.*, 2015    |
| **DecMeg2014 (Kaggle)**   | 23            | Elekta Triux 306ch      | Face recognition (2 classes, cross-subject)            | Huttunen *et al.*, 2014 |


## Models

| **Study (Year)**          | **Task**                                  | **Architecture/Method**                      | **Key Result**                                                                                            |
| ------------------------- | ----------------------------------------- | -------------------------------------------- | --------------------------------------------------------------------------------------------------------- |
| **Cichy et al., 2017**    | Scene image viewing (multiple categories) | RSA: DNN (AlexNet) layer vs MEG at each time | MEG correlates with deep CNN layers; category info emerges ~100 ms post-stimulus.                         |
| **Seeliger et al., 2018** | Object recognition (4 classes)            | CNN encoder/decoder (space–time features)    | ~80% accuracy for 4-way object category; spatiotemporal patterns mapped to convnet features.              |
| **Cecotti 2016**          | RSVP target detection (dual stream)       | Single-trial SVM on MEG ERFs                 | ~85% detection accuracy for target vs non-target; reliable single-trial oddball detection with MEG.       |
| **Li et al., 2021**       | Faces vs Scrambled (binary)               | Hybrid GRU (HGRN) inter-subject decoder      | ~70% accuracy (LOO across 16 subjects), besting prior logistic regression (60–65%).                       |
| **Wittevrongel 2021**     | “Mind-spelling” SSVEP (6 targets)         | 50-channel OPM-MEG + adaptive classification | 97.7% spelling accuracy; robust single-trial SSVEP detection in real time.                                |
| **Meta AI 2024**          | Natural images (20k, 8 classes subset)    | Image emb. regression + Diffusion generator  | Top-5 image retrieval ~70%; successful semantic image reconstructions from MEG signals (~300 ms latency). |


## Devices


# Hybrid FNIRS + EEG

| **Study**                        | **Modalities**            | **Task / Application**                             | **Key Findings**                                                                                                                                                                                                                                   |
| -------------------------------- | ------------------------- | -------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Wang *et al.*, 2025              | EEG + fNIRS (hybrid)      | Motor Imagery (left/right hand; grasp vs rest)     | Proposed a dynamic GCN + Capsule network. Achieved ~92% MI accuracy, outperforming single-modality models by >4%. Cross-modal attention fusion improved robustness across sessions and subjects.                                                   |
| Chiarelli *et al.*, 2018         | EEG + fNIRS               | Motor Imagery (left vs right hand)                 | Early deep learning approach: a simple DNN trained on hybrid features beat EEG-only and fNIRS-only classification. Demonstrated complementary nature of EEG and hemodynamic signals for MI.                                                        |
| Khan *et al.*, 2020 (reviewed)   | EEG + fNIRS + others      | Neuroergonomics (various, e.g. driving simulators) | Surveys 33 studies showing hybrid EEG-fNIRS yields higher mental state decoding accuracy than EEG or fNIRS alone. Noted improvements in domains like vigilance and mental workload through data fusion.                                            |
| Hu *et al.*, 2025                | EEG + fNIRS               | Multi-task (public datasets: workload & MI)        | Developed CNN-Transformer hybrid model. On 3 datasets, hybrid fusion outperformed unimodal CNN or Transformer, highlighting that temporal-spatial features from both modalities are complementary.                                                 |
| Putze *et al.*, 2014 (TU Berlin) | EEG + fNIRS (+ eye-track) | Auditory vs Visual workload tasks (multiclass)     | Pioneering work on hybrid BCI for multiple mental tasks. Achieved higher classification accuracy by combining modalities (EEG detected fast responses; fNIRS differentiated tasks via PFC activation). Released an open dataset for the community. |

